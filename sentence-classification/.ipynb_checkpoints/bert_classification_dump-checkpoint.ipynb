{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the BERT_RNN_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
      "\u001b[K     |████████████████████████████████| 563 kB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /root/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.10)\n",
      "Collecting tokenizers==0.5.2\n",
      "  Downloading tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 17.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /root/anaconda3/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.4.4-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
      "\u001b[K     |████████████████████████████████| 679 kB 41.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 55.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /root/anaconda3/lib/python3.7/site-packages (from transformers) (4.45.0)\n",
      "Requirement already satisfied: boto3 in /root/anaconda3/lib/python3.7/site-packages (from transformers) (1.12.36)\n",
      "Requirement already satisfied: requests in /root/anaconda3/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\n",
      "\u001b[K     |████████████████████████████████| 860 kB 50.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /root/anaconda3/lib/python3.7/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /root/anaconda3/lib/python3.7/site-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.36 in /root/anaconda3/lib/python3.7/site-packages (from boto3->transformers) (1.15.36)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /root/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /root/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /root/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.25.7)\n",
      "Requirement already satisfied: six in /root/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in /root/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /root/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.13.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /root/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.36->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /root/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.36->boto3->transformers) (0.15.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884629 sha256=2ca059e5e41550c75d2fc77466caf7cca029f6b70ca4b0c7df0f0e9defa37a55\n",
      "  Stored in directory: /root/.cache/pip/wheels/99/c9/5a/a5e36bce983040ea5061a8ec65b5852bfebad4b1afa8d5b394\n",
      "Successfully built sacremoses\n",
      "\u001b[33mWARNING: Error parsing requirements for cord19: Parse error at \"'(===1.4.'\": Expected stringEnd\u001b[0m\n",
      "Installing collected packages: tokenizers, regex, sentencepiece, sacremoses, transformers\n",
      "Successfully installed regex-2020.4.4 sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b56791c502e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from nltk1.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def inputid (inputsent,tokenizername):\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizername)\n",
    "    input_ids = []\n",
    "    for sent in tqdm(inputsent):\n",
    "        sent= word_tokenize(sent)[0:500]\n",
    "        encoded_sent = tokenizer.encode(sent,add_special_tokens = True)\n",
    "        input_ids.append(encoded_sent)\n",
    "    return input_ids\n",
    "\n",
    "def maxwordnum(allsec):\n",
    "    allsentlen=[]\n",
    "    for i in tqdm(allsec):\n",
    "        wordnu=len(i)\n",
    "        allsentlen.append(wordnu)\n",
    "    maxnum=max(np.array(allsentlen))\n",
    "    return maxnum\n",
    "\n",
    "def dxseqpadding (seq,maxnu):\n",
    "    seq2=[]\n",
    "    for i in tqdm(seq):\n",
    "        stamp=len(i)\n",
    "        i=np.pad(i,((0,maxnu-stamp)),'constant',constant_values=0)\n",
    "        seq2.append(i)\n",
    "    return seq2\n",
    "\n",
    "def attid (inputsent):\n",
    "    attention_masks = []\n",
    "    for sent in tqdm(inputsent):\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "    return attention_masks\n",
    "\n",
    "def dataloader (trainval, test,args):\n",
    "    train_inputs=trainval[0]\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    train_labels=trainval[1]\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    train_masks=trainval[2]\n",
    "    train_masks = torch.tensor(train_masks)\n",
    "    \n",
    "    val_inputs=trainval[3]\n",
    "    val_inputs = torch.tensor(val_inputs)\n",
    "    val_labels=trainval[4]\n",
    "    val_labels = torch.tensor(val_labels)\n",
    "    val_masks=trainval[5]\n",
    "    val_masks = torch.tensor(val_masks)\n",
    "    \n",
    "    test_inputs=test[0]\n",
    "    test_inputs = torch.tensor(test_inputs)\n",
    "    test_labels=test[1]\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    test_masks=test[2]\n",
    "    test_masks = torch.tensor(test_masks)\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)    \n",
    "    train_dataloader = DataLoader(train_data, batch_size=args)\n",
    "    \n",
    "    validation_data = TensorDataset(val_inputs, val_masks, val_labels)    \n",
    "    validation_dataloader = DataLoader(validation_data, batch_size=args)\n",
    "        \n",
    "    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=args)\n",
    "    \n",
    "    return (train_dataloader,validation_dataloader,test_dataloader)\n",
    "    \n",
    "def bertrnn_process (args,trainsent,valsent,testsent,trainlabel,vallabel,testlabel):\n",
    "    if args['science']==True:       \n",
    "        trainsci=inputid(trainsent,args['modelname2'])\n",
    "        valsci=inputid(valsent,args['modelname2'])\n",
    "        testsci=inputid(testsent,args['modelname2'])\n",
    "        trainnor=inputid(trainsent,args['modelname1'])\n",
    "        valnor=inputid(valsent,args['modelname1'])\n",
    "        testnor=inputid(testsent,args['modelname1'])\n",
    "        maxnum=maxwordnum(testnor)\n",
    "        trainsci=dxseqpadding(trainsci,maxnum)\n",
    "        valsci=dxseqpadding(valsci,maxnum)\n",
    "        testsci=dxseqpadding(testsci,maxnum)\n",
    "        trainnor=dxseqpadding(trainnor,maxnum)\n",
    "        valnor=dxseqpadding(valnor,maxnum)\n",
    "        testnor=dxseqpadding(testnor,maxnum)\n",
    "        trainsciatt=attid(trainsci)\n",
    "        valsciatt=attid(valsci)\n",
    "        testsciatt=attid(testsci)\n",
    "        trainnoratt=attid(trainnor)\n",
    "        valnoratt=attid(valnor)\n",
    "        testnoratt=attid(testnor)\n",
    "        nortrainval=(trainnor,trainlabel,trainnoratt,valnor,vallabel,valnoratt)\n",
    "        scitrainval=(trainsci,trainlabel,trainsciatt,valsci,vallabel,valsciatt)\n",
    "        scitest=(testsci,testlabel,testsciatt)\n",
    "        nortest=(testnor,testlabel,testnoratt)\n",
    "        norloder=dataloader (nortrainval, nortest,int(args['batch_size']))\n",
    "        sciloder=dataloader (scitrainval, scitest,int(args['batch_size']))\n",
    "    else : \n",
    "            \n",
    "        trainnor=inputid(trainsent,args['modelname1'])\n",
    "        valnor=inputid(valsent,args['modelname1'])\n",
    "        testnor=inputid(testsent,args['modelname1'])\n",
    "        maxnum=maxwordnum(testnor)\n",
    "        trainnor=dxseqpadding(trainnor,maxnum)\n",
    "        valnor=dxseqpadding(valnor,maxnum)\n",
    "        testnor=dxseqpadding(testnor,maxnum)\n",
    "        trainnoratt=attid(trainnor)\n",
    "        valnoratt=attid(valnor)\n",
    "        testnoratt=attid(testnor)\n",
    "        nortrainval=(trainnor,trainlabel,trainnoratt,valnor,vallabel,valnoratt)        \n",
    "        nortest=(testnor,testlabel,testnoratt)\n",
    "        norloder=dataloader (nortrainval, nortest,int(args['batch_size']))       \n",
    "        sciloder=[]\n",
    "    return norloder,sciloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/text_download/keylist.txt', \"r\") as f:\n",
    "    alist =f.read().splitlines()\n",
    "    for line in alist:\n",
    "        keylist=line.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsent=pd.read_csv('./trainsent.csv').sent.values\n",
    "valsent=pd.read_csv('./valsent.csv').sent.values\n",
    "testsent=pd.read_csv('./testsent.csv').sent.values\n",
    "trainlabel=pd.read_csv('./trainlabel.csv')[keylist].values\n",
    "vallabel=pd.read_csv('./vallabel.csv')[keylist].values\n",
    "testlabel=pd.read_csv('./testlabel.csv').newpid.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "startw=np.count_nonzero(trainlabel== 1, axis=0)\n",
    "defualt=np.max(startw)\n",
    "squarer = lambda t: defualt/t\n",
    "trainweight=np.array([squarer(xi) for xi in startw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\"modelname1\":'bert-base-uncased',\n",
    "      \"modelname2\":'scibert_scivocab_uncased',\n",
    "      \"num_labels\":len(trainlabel[0]),\n",
    "      \"hidden_size\":256,\n",
    "      \"num_layers\":1,\n",
    "      \"bidirectional\":1,\n",
    "      \"dropout\":0.2,\n",
    "      \"batch_size\":10,\n",
    "      \"epochs\":100,\n",
    "      \"lr\":0.001,\n",
    "      \"seed\": 1,\n",
    "      \"early_stop_times\":5,\n",
    "      \"science\":True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norloder,sciloder=bertrnn_process(args,trainsent,valsent,testsent,trainlabel,vallabel,testlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is model and running code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import label_ranking_loss\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from transformers import BertModel, BertConfig, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "class bert_rnn(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(bert_rnn, self).__init__()\n",
    "        self.args = args\n",
    "        self.emb1=BertModel.from_pretrained(self.args['modelname1'],num_labels = self.args['num_labels'],output_attentions = False,output_hidden_states = False)#.cuda(3)\n",
    "        self.emb1_size=self.emb1.config.hidden_size\n",
    "        if self.args['science']==True:\n",
    "            self.emb2=BertModel.from_pretrained(self.args['modelname2'],num_labels = self.args['num_labels'],output_attentions = False,output_hidden_states = False)#.cuda(3)\n",
    "            self.emb2_size=self.emb2.config.hidden_size\n",
    "            self.emb_size=self.emb1_size+self.emb2_size\n",
    "        else:\n",
    "            self.emb_size=self.emb1_size\n",
    "        self.lin1 = nn.Linear(self.emb_size, self.args['hidden_size'])\n",
    "        if self.args['bidirectional']>1:\n",
    "            bidirectional=2\n",
    "            dif=True\n",
    "        else:\n",
    "            bidirectional=1\n",
    "            dif=False\n",
    "        self.rnn=nn.LSTM(input_size=self.args['hidden_size'], hidden_size =self.args['hidden_size'], num_layers =self.args['num_layers'],batch_first= True,dropout=self.args['dropout'],bidirectional=dif)\n",
    "        self.lin2 = nn.Linear(self.args['hidden_size'], self.args['num_labels'])\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    def forward(self,data1,mask1,target,data2=None,mask2=None):\n",
    "        if self.args['science']==True:\n",
    "            emb1=self.emb1(data1,attention_mask=mask1)\n",
    "            last_hidden_states1 = emb1[0]\n",
    "            emb2=self.emb2(data2,attention_mask=mask2)\n",
    "            last_hidden_states2 = emb2[0]\n",
    "            last_hidden_states = torch.cat((last_hidden_states1, last_hidden_states2), 2)\n",
    "            last_hidden_states = self.lin1(last_hidden_states)\n",
    "        else:\n",
    "            emb1=self.emb1(data1)\n",
    "            last_hidden_states1 = emb1[0]\n",
    "            last_hidden_states = self.lin1(last_hidden_states1)\n",
    "        output, _ = self.rnn(last_hidden_states)#,# (self.h0, self.c0))\n",
    "        output=torch.mean(output, 1)\n",
    "        out = self.lin2(output)\n",
    "        out=self.sigmoid(out)\n",
    "        return out,target\n",
    "    def parameters (self):\n",
    "        if self.args['science']==True:\n",
    "            params=list(self.emb1.parameters())+list(self.emb2.parameters())+list(self.rnn.parameters())+list(self.lin1.parameters())+list(self.lin2.parameters())\n",
    "        else:\n",
    "            params=list(self.emb1.parameters())+list(self.rnn.parameters())+list(self.lin1.parameters())+list(self.lin2.parameters())\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_(model,loss,optimizer,dataloaders1,epoch,dataloaders2=None):\n",
    "    allloss=[]\n",
    "    allbatch=[]\n",
    "    print('Train')\n",
    "    if args['science']==True:\n",
    "        for batch_idx, batch in tqdm(enumerate(zip(dataloaders1, dataloaders2))):\n",
    "            data1, mask1, target1 = batch[0]\n",
    "            data2, mask2, target2 = batch[1]\n",
    "            target1 = Variable(target1).cuda()\n",
    "            data1 = Variable(data1).cuda()\n",
    "            mask1=Variable(mask1).cuda()\n",
    "            target2 = Variable(target2).cuda()\n",
    "            data2 = Variable(data2).cuda()\n",
    "            mask2=Variable(mask2).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            out,target = model.forward(data1,mask1,target1,data2,mask2)\n",
    "            lossall = loss(out,target.float())            \n",
    "            lossall = torch.sum(lossall)\n",
    "            lossall.backward()\n",
    "            optimizer.step()\n",
    "            loss1=lossall.item()\n",
    "            allloss.append(loss1)\n",
    "            allbatch.append(batch_idx*epoch)\n",
    "    else:\n",
    "        for batch_idx, batch in tqdm(enumerate(dataloaders1)):\n",
    "            data1, mask1, target1 = batch\n",
    "            target1 = Variable(target1).cuda(3)\n",
    "            data1 = Variable(data1).cuda(3)\n",
    "            mask1=Variable(mask1).cuda(3)\n",
    "            optimizer.zero_grad()\n",
    "            out,target = model.forward(data1,mask1,target1)\n",
    "            lossall = loss(out,target.float())            \n",
    "            lossall = torch.sum(lossall)\n",
    "            lossall.backward()\n",
    "            optimizer.step()\n",
    "            allloss.append(lossall)\n",
    "            allbatch.append(batch_idx*epoch)\n",
    "    return allloss, allbatch\n",
    "def val_(model,dataloaders1,dataloaders2=None):\n",
    "    allout=[]\n",
    "    alltarget=[]\n",
    "    print('Validation')\n",
    "    if args['science']==True:\n",
    "        for batch_idx, batch in tqdm(enumerate(zip(dataloaders1, dataloaders2))):\n",
    "            data1, mask1, target1 = batch[0]\n",
    "            data2, mask2, target2 = batch[1]\n",
    "            target1 = Variable(target1).cuda()\n",
    "            data1 = Variable(data1).cuda()\n",
    "            mask1=Variable(mask1).cuda()\n",
    "            target2 = Variable(target2).cuda()\n",
    "            data2 = Variable(data2).cuda()\n",
    "            mask2=Variable(mask2).cuda()\n",
    "            with torch.no_grad(): \n",
    "                out,target = model.forward(data1,mask1,target1,data2,mask2)\n",
    "            out=out.cpu().detach().numpy()\n",
    "            target=target.to('cpu').numpy().astype(int)\n",
    "            \n",
    "            allout.append(list(out))\n",
    "            alltarget.append(list(target))\n",
    "    else:\n",
    "        for batch_idx, batch in tqdm(enumerate(dataloaders1)):\n",
    "            data1, mask1, target1 = batch\n",
    "            target1 = Variable(target1).cuda()\n",
    "            data1 = Variable(data1).cuda()\n",
    "            mask1=Variable(mask1).cuda()\n",
    "            with torch.no_grad(): \n",
    "                out,target = model.forward(data1,mask1,target1)\n",
    "            out=out.cpu().detach().numpy()\n",
    "            target=target.to('cpu').numpy().astype(int)\n",
    "            \n",
    "            allout.append(list(out))\n",
    "            alltarget.append(list(target))\n",
    "    return allout,alltarget\n",
    "def test_(model,dataloaders1,dataloaders2=None):\n",
    "    allout=[]\n",
    "    alltarget=[]\n",
    "    print('test')\n",
    "    if args['science']==True:\n",
    "        for batch_idx, batch in tqdm(enumerate(zip(dataloaders1, dataloaders2))):\n",
    "            data1, mask1, target1 = batch[0]\n",
    "            data2, mask2, target2 = batch[1]\n",
    "            target1 = Variable(target1).cuda()\n",
    "            data1 = Variable(data1).cuda()\n",
    "            mask1=Variable(mask1).cuda()\n",
    "            target2 = Variable(target2).cuda()\n",
    "            data2 = Variable(data2).cuda()\n",
    "            mask2=Variable(mask2).cuda()\n",
    "            with torch.no_grad(): \n",
    "                out,target = model.forward(data1,mask1,target1,data2,mask2)\n",
    "            out=out.cpu().detach().numpy()\n",
    "            target=target.to('cpu').numpy().astype(int)\n",
    "            \n",
    "            allout.append(out)\n",
    "            alltarget.append(target)\n",
    "    else:\n",
    "        for batch_idx, batch in tqdm(enumerate(dataloaders1)):\n",
    "            data1, mask1, target1 = batch\n",
    "            target1 = Variable(target1).cuda()\n",
    "            data1 = Variable(data1).cuda()\n",
    "            mask1=Variable(mask1).cuda()\n",
    "            with torch.no_grad(): \n",
    "                out,target = model.forward(data1,mask1,target1)\n",
    "            out=out.cpu().detach().numpy()\n",
    "            target=target.to('cpu').numpy().astype(int)\n",
    "            \n",
    "            allout.append(out)\n",
    "            alltarget.append(target)\n",
    "    return allout,alltarget\n",
    "def result(result):\n",
    "    alllist=[]\n",
    "    for i in result:\n",
    "        for j in i:\n",
    "            alllist.append(j)\n",
    "    return alllist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "torch.cuda.set_device(0)\n",
    "model= bert_rnn(args)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model=nn.DataParallel(model, device_ids=[0,1,2,4,5])\n",
    "model.to(device)\n",
    "params=model.parameters()\n",
    "optimizer = AdamW(params,lr = 2e-5, eps = 1e-8 )\n",
    "trainweight=torch.tensor(trainweight).float().cuda()\n",
    "loss = nn.BCEWithLogitsLoss(pos_weight=trainweight)\n",
    "#loss = nn.MSELoss()\n",
    "alloss=[]\n",
    "allbatch=[]\n",
    "dev_lrl=1\n",
    "vallrl=[]\n",
    "testpred=[]\n",
    "testpid=[]\n",
    "current_early_stop_times=0\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    epochloss, epochbatch=train_(model,loss,optimizer,norloder[0],epoch,dataloaders2=sciloder[0])\n",
    "    alloss.append(epochloss)\n",
    "    allbatch.append(epochbatch)\n",
    "    allout,alltarget=val_(model,norloder[1],sciloder[1])\n",
    "    allout1=result(allout)\n",
    "    alltarget1=result(alltarget)\n",
    "    epochlrl=label_ranking_loss(alltarget1,allout1)\n",
    "    vallrl.append(epochlrl)\n",
    "    if epochlrl < dev_lrl:\n",
    "        print(\"- new best lrl{}\".format(epochlrl))\n",
    "        allout,alltarget=test_(model,norloder[2],sciloder[2])\n",
    "        allout=result(allout)\n",
    "        alltarget=result(alltarget)\n",
    "        allpd=pd.DataFrame(allout,columns=keylist)\n",
    "        allpd['newpid']=alltarget\n",
    "        allpd.to_csv('Bert_NIH_task4_Q1_LSTM.csv')\n",
    "        dev_lrl = epochlrl\n",
    "        current_early_stop_times = 0\n",
    "    else:\n",
    "        current_early_stop_times += 1\n",
    "        print(current_early_stop_times)\n",
    "    if current_early_stop_times >= args['early_stop_times'] :\n",
    "        break;\n",
    "print (\"- early stopping {} epochs without improvement\".format(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
