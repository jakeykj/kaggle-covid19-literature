{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "question='Q1/'\n",
    "args={\"input_file_dir\": '../snorkel-pseudo-label/'+question,\n",
    "      \"output_file_dir\": question,\n",
    "      \"modelname1\":'bert-base-uncased', \n",
    "      \"modelname2\":'allenai/scibert_scivocab_uncased',\n",
    "      \"hidden_size\":256,\n",
    "      \"dropout\":0.2,\n",
    "      \"batch_size\":10,\n",
    "      \"epochs\":100,\n",
    "      \"lr\":0.001,\n",
    "      \"seed\": 1,\n",
    "      \"early_stop_times\":5,\n",
    "      \"science\":True,\n",
    "      \"keylist\":'keylist.txt', # for each question's keylist\n",
    "      \"sentence_ranking_file\": 'Bert_NIH_task4.csv', #file name to save sentence_ranking \n",
    "      \"device\":0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 18:10:06.574221 140255500728064 tokenization_utils.py:420] Model name 'allenai/scibert_scivocab_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'allenai/scibert_scivocab_uncased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0413 18:10:07.288724 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/vocab.txt from cache at /root/.cache/torch/transformers/e3debd8fbdf40874753724814ee0520f612b577b26c8755bca485103b47cd3bc.60287becc5ab96d85a4bf377eb90feaf3b9c80d3b23e84311dccd3588f56d4fb\n",
      "I0413 18:10:07.290483 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/added_tokens.json from cache at None\n",
      "I0413 18:10:07.291787 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/special_tokens_map.json from cache at None\n",
      "I0413 18:10:07.292861 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/tokenizer_config.json from cache at None\n",
      "100%|██████████| 656/656 [00:00<00:00, 2868.14it/s]\n",
      "I0413 18:10:07.588967 140255500728064 tokenization_utils.py:420] Model name 'allenai/scibert_scivocab_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'allenai/scibert_scivocab_uncased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0413 18:10:08.711726 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/vocab.txt from cache at /root/.cache/torch/transformers/e3debd8fbdf40874753724814ee0520f612b577b26c8755bca485103b47cd3bc.60287becc5ab96d85a4bf377eb90feaf3b9c80d3b23e84311dccd3588f56d4fb\n",
      "I0413 18:10:08.712710 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/added_tokens.json from cache at None\n",
      "I0413 18:10:08.713816 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/special_tokens_map.json from cache at None\n",
      "I0413 18:10:08.714792 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/tokenizer_config.json from cache at None\n",
      "100%|██████████| 165/165 [00:00<00:00, 2414.56it/s]\n",
      "I0413 18:10:08.834375 140255500728064 tokenization_utils.py:420] Model name 'allenai/scibert_scivocab_uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'allenai/scibert_scivocab_uncased' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0413 18:10:09.559159 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/vocab.txt from cache at /root/.cache/torch/transformers/e3debd8fbdf40874753724814ee0520f612b577b26c8755bca485103b47cd3bc.60287becc5ab96d85a4bf377eb90feaf3b9c80d3b23e84311dccd3588f56d4fb\n",
      "I0413 18:10:09.560207 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/added_tokens.json from cache at None\n",
      "I0413 18:10:09.561280 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/special_tokens_map.json from cache at None\n",
      "I0413 18:10:09.563282 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/tokenizer_config.json from cache at None\n",
      "100%|██████████| 25739/25739 [00:04<00:00, 5183.89it/s]\n",
      "I0413 18:10:14.737885 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "100%|██████████| 656/656 [00:00<00:00, 3977.54it/s]\n",
      "I0413 18:10:15.127583 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "100%|██████████| 165/165 [00:00<00:00, 2269.76it/s]\n",
      "I0413 18:10:15.429189 140255500728064 tokenization_utils.py:504] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "100%|██████████| 25739/25739 [00:04<00:00, 5522.38it/s]\n",
      "100%|██████████| 25739/25739 [00:00<00:00, 3448340.33it/s]\n",
      "100%|██████████| 656/656 [00:00<00:00, 32479.06it/s]\n",
      "100%|██████████| 165/165 [00:00<00:00, 30145.93it/s]\n",
      "100%|██████████| 25739/25739 [00:00<00:00, 33555.91it/s]\n",
      "100%|██████████| 656/656 [00:00<00:00, 39778.28it/s]\n",
      "100%|██████████| 165/165 [00:00<00:00, 28520.92it/s]\n",
      "100%|██████████| 25739/25739 [00:00<00:00, 35504.27it/s]\n",
      "100%|██████████| 656/656 [00:00<00:00, 12031.20it/s]\n",
      "100%|██████████| 165/165 [00:00<00:00, 10904.08it/s]\n",
      "100%|██████████| 25739/25739 [00:02<00:00, 11569.64it/s]\n",
      "100%|██████████| 656/656 [00:00<00:00, 11359.92it/s]\n",
      "100%|██████████| 165/165 [00:00<00:00, 11019.01it/s]\n",
      "100%|██████████| 25739/25739 [00:02<00:00, 11465.60it/s]\n",
      "I0413 18:10:27.572777 140255500728064 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0413 18:10:27.574962 140255500728064 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 8,\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0413 18:10:27.750638 140255500728064 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0413 18:10:30.207480 140255500728064 configuration_utils.py:283] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/config.json from cache at /root/.cache/torch/transformers/199e28e62d2210c23d63625bd9eecc20cf72a156b29e2a540d4933af4f50bda1.79c4dd84b76a6991002b44cd58102c732c37aba834ad6401ddd6a89bd0ed809b\n",
      "I0413 18:10:30.209053 140255500728064 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 8,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "I0413 18:10:30.376672 140255500728064 modeling_utils.py:507] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/allenai/scibert_scivocab_uncased/pytorch_model.bin from cache at /root/.cache/torch/transformers/a4e19031683f34af5fd1c4cca73a3dbe33f8b9e50ad91ddf12ceac577b93c433.7587182ea55c40bf7fd0961c1176c31fa22558da2bf20c199874fa5a8ecb4613\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.79it/s]\n",
      "2it [00:00, 13.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.50it/s]\n",
      "2it [00:00, 13.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- new best lrl0.1541125541125541\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2574it [03:16, 13.10it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.82it/s]\n",
      "2it [00:00, 13.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.45it/s]\n",
      "2it [00:00, 11.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- new best lrl0.14718614718614717\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2574it [03:18, 12.99it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.82it/s]\n",
      "2it [00:00, 13.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.41it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.81it/s]\n",
      "2it [00:00, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.46it/s]\n",
      "2it [00:00, 13.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- new best lrl0.12294372294372291\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2574it [03:16, 13.09it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.83it/s]\n",
      "2it [00:00, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.47it/s]\n",
      "2it [00:00, 13.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- new best lrl0.07012987012987011\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2574it [03:16, 13.12it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.82it/s]\n",
      "2it [00:00, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.50it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.82it/s]\n",
      "2it [00:00, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.50it/s]\n",
      "2it [00:00, 13.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- new best lrl0.06926406926406925\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2574it [03:16, 13.10it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.81it/s]\n",
      "2it [00:00, 13.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.46it/s]\n",
      "2it [00:00, 13.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- new best lrl0.06753246753246753\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2574it [03:16, 13.08it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.82it/s]\n",
      "2it [00:00, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.49it/s]\n",
      "2it [00:00, 13.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- new best lrl0.06666666666666667\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2574it [03:16, 13.10it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.82it/s]\n",
      "2it [00:00, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.49it/s]\n",
      "2it [00:00, 13.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- new best lrl0.062337662337662345\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2574it [03:16, 13.11it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.82it/s]\n",
      "2it [00:00, 13.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.47it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.85it/s]\n",
      "2it [00:00, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.52it/s]\n",
      "2it [00:00, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- new best lrl0.06233766233766233\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2574it [03:15, 13.14it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.85it/s]\n",
      "2it [00:00, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.50it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.84it/s]\n",
      "2it [00:00, 13.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.51it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.85it/s]\n",
      "2it [00:00, 13.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.51it/s]\n",
      "2it [00:00, 13.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- new best lrl0.04675324675324675\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2574it [03:16, 13.12it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.85it/s]\n",
      "2it [00:00, 13.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.54it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.86it/s]\n",
      "2it [00:00, 13.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.48it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.85it/s]\n",
      "2it [00:00, 13.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.49it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.85it/s]\n",
      "2it [00:00, 13.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.49it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:17,  3.85it/s]\n",
      "2it [00:00, 13.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:01, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "- early stopping 20 epochs without improvement\n"
     ]
    }
   ],
   "source": [
    "#!pip install tqdm boto3 requests regex sentencepiece sacremoses\n",
    "#!pip install transformers\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import label_ranking_loss\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from transformers import BertTokenizer,BertModel, BertConfig, AdamW\n",
    "\n",
    "    \n",
    "def inputid (inputsent,tokenizername):\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizername)\n",
    "    input_ids = []\n",
    "    for sent in tqdm(inputsent):\n",
    "        sent= word_tokenize(sent)[0:500]\n",
    "        encoded_sent = tokenizer.encode(sent,add_special_tokens = True)\n",
    "        input_ids.append(encoded_sent)\n",
    "    return input_ids\n",
    "\n",
    "def maxwordnum(allsec):\n",
    "    allsentlen=[]\n",
    "    for i in tqdm(allsec):\n",
    "        wordnu=len(i)\n",
    "        allsentlen.append(wordnu)\n",
    "    maxnum=max(np.array(allsentlen))\n",
    "    return maxnum\n",
    "\n",
    "def dxseqpadding (seq,maxnu):\n",
    "    seq2=[]\n",
    "    for i in tqdm(seq):\n",
    "        stamp=len(i)\n",
    "        i=np.pad(i,((0,maxnu-stamp)),'constant',constant_values=0)\n",
    "        seq2.append(i)\n",
    "    return seq2\n",
    "\n",
    "def attid (inputsent):\n",
    "    attention_masks = []\n",
    "    for sent in tqdm(inputsent):\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "    return attention_masks\n",
    "\n",
    "def dataloader (trainval, test,args):\n",
    "    train_inputs=trainval[0]\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    train_labels=trainval[1]\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    train_masks=trainval[2]\n",
    "    train_masks = torch.tensor(train_masks)\n",
    "    \n",
    "    val_inputs=trainval[3]\n",
    "    val_inputs = torch.tensor(val_inputs)\n",
    "    val_labels=trainval[4]\n",
    "    val_labels = torch.tensor(val_labels)\n",
    "    val_masks=trainval[5]\n",
    "    val_masks = torch.tensor(val_masks)\n",
    "    \n",
    "    test_inputs=test[0]\n",
    "    test_inputs = torch.tensor(test_inputs)\n",
    "    test_labels=test[1]\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    test_masks=test[2]\n",
    "    test_masks = torch.tensor(test_masks)\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)    \n",
    "    train_dataloader = DataLoader(train_data, batch_size=args, shuffle=True)\n",
    "    \n",
    "    validation_data = TensorDataset(val_inputs, val_masks, val_labels)    \n",
    "    validation_dataloader = DataLoader(validation_data, batch_size=args, shuffle=True)\n",
    "        \n",
    "    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=args, shuffle=True)\n",
    "    \n",
    "    return (train_dataloader,validation_dataloader,test_dataloader)\n",
    "    \n",
    "def bertrnn_process (args,trainsent,valsent,testsent,trainlabel,vallabel,testlabel):\n",
    "    if args['science']==True:       \n",
    "        trainsci=inputid(trainsent,args['modelname2'])\n",
    "        valsci=inputid(valsent,args['modelname2'])\n",
    "        testsci=inputid(testsent,args['modelname2'])\n",
    "        trainnor=inputid(trainsent,args['modelname1'])\n",
    "        valnor=inputid(valsent,args['modelname1'])\n",
    "        testnor=inputid(testsent,args['modelname1'])\n",
    "        maxnum=maxwordnum(testnor)\n",
    "        trainsci=dxseqpadding(trainsci,maxnum)\n",
    "        valsci=dxseqpadding(valsci,maxnum)\n",
    "        testsci=dxseqpadding(testsci,maxnum)\n",
    "        trainnor=dxseqpadding(trainnor,maxnum)\n",
    "        valnor=dxseqpadding(valnor,maxnum)\n",
    "        testnor=dxseqpadding(testnor,maxnum)\n",
    "        trainsciatt=attid(trainsci)\n",
    "        valsciatt=attid(valsci)\n",
    "        testsciatt=attid(testsci)\n",
    "        trainnoratt=attid(trainnor)\n",
    "        valnoratt=attid(valnor)\n",
    "        testnoratt=attid(testnor)\n",
    "        nortrainval=(trainnor,trainlabel,trainnoratt,valnor,vallabel,valnoratt)\n",
    "        scitrainval=(trainsci,trainlabel,trainsciatt,valsci,vallabel,valsciatt)\n",
    "        scitest=(testsci,testlabel,testsciatt)\n",
    "        nortest=(testnor,testlabel,testnoratt)\n",
    "        norloder=dataloader(nortrainval, nortest,int(args['batch_size']))\n",
    "        sciloder=dataloader(scitrainval, scitest,int(args['batch_size']))\n",
    "    else : \n",
    "            \n",
    "        trainnor=inputid(trainsent,args['modelname1'])\n",
    "        valnor=inputid(valsent,args['modelname1'])\n",
    "        testnor=inputid(testsent,args['modelname1'])\n",
    "        maxnum=maxwordnum(testnor)\n",
    "        trainnor=dxseqpadding(trainnor,maxnum)\n",
    "        valnor=dxseqpadding(valnor,maxnum)\n",
    "        testnor=dxseqpadding(testnor,maxnum)\n",
    "        trainnoratt=attid(trainnor)\n",
    "        valnoratt=attid(valnor)\n",
    "        testnoratt=attid(testnor)\n",
    "        nortrainval=(trainnor,trainlabel,trainnoratt,valnor,vallabel,valnoratt)        \n",
    "        nortest=(testnor,testlabel,testnoratt)\n",
    "        norloder=dataloader(nortrainval, nortest,int(args['batch_size']))       \n",
    "        sciloder=[]\n",
    "    return norloder,sciloder\n",
    "\n",
    "\n",
    "input_directory=args['input_file_dir']\n",
    "with open(input_directory+args['keylist'], \"r\") as f:\n",
    "    alist =f.read().splitlines()\n",
    "    for line in alist:\n",
    "        keylist=line.split(',')\n",
    "\n",
    "\n",
    "\n",
    "trainsent=pd.read_csv(input_directory+'trainsent.csv').sent.values\n",
    "valsent=pd.read_csv(input_directory+'valsent.csv').sent.values\n",
    "testsent=pd.read_csv(input_directory+'testsent.csv').sent.values\n",
    "trainlabel=pd.read_csv(input_directory+'trainlabel.csv')[keylist].values\n",
    "vallabel=pd.read_csv(input_directory+'vallabel.csv')[keylist].values\n",
    "testlabel=pd.read_csv(input_directory+'testlabel.csv').newpid.values\n",
    "\n",
    "num_labels=len(trainlabel[0])\n",
    "\n",
    "\n",
    "startw=np.count_nonzero(trainlabel== 1, axis=0)\n",
    "defualt=np.max(startw)\n",
    "squarer = lambda t: defualt/t\n",
    "trainweight=np.array([squarer(xi) for xi in startw])\n",
    "\n",
    "\n",
    "\n",
    "norloder,sciloder=bertrnn_process(args,trainsent,valsent,testsent,trainlabel,vallabel,testlabel)\n",
    "\n",
    "\n",
    "\n",
    "class bert_rnn(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(bert_rnn, self).__init__()\n",
    "        self.args = args\n",
    "        self.emb1=BertModel.from_pretrained(self.args['modelname1'],num_labels = num_labels,output_attentions = False,output_hidden_states = False)#.cuda(3)\n",
    "        self.emb1_size=self.emb1.config.hidden_size\n",
    "        if self.args['science']==True:\n",
    "            self.emb2=BertModel.from_pretrained(self.args['modelname2'],num_labels = num_labels,output_attentions = False,output_hidden_states = False)#.cuda(3)\n",
    "            self.emb2_size=self.emb2.config.hidden_size\n",
    "            self.emb_size=self.emb1_size+self.emb2_size\n",
    "        else:\n",
    "            self.emb_size=self.emb1_size\n",
    "        #self.pool = nn.MaxPool1d(args['pool_kernel_size'])\n",
    "        #self.bn = nn.BatchNorm1d(self.emb_size)\n",
    "        self.lin1 = nn.Linear(self.emb_size, self.args['hidden_size'])\n",
    "        self.dropout = nn.Dropout(args['dropout'])\n",
    "        self.lin2 = nn.Linear(self.args['hidden_size'], num_labels)\n",
    "        \n",
    "    def forward(self,data1,mask1,data2=None,mask2=None):\n",
    "        if self.args['science']==True:\n",
    "            emb1=self.emb1(data1,attention_mask=mask1)\n",
    "            #pooler_output1=torch.mean(emb1[0], 1)\n",
    "            #pooler_output1=torch.mean(self.pool(emb1[0].transpose(2,1)),2)\n",
    "            pooler_output1 = emb1[1]\n",
    "            emb2=self.emb2(data2,attention_mask=mask2)\n",
    "            #pooler_output2=torch.mean(emb2[0], 1)\n",
    "            #pooler_output2=torch.mean(self.pool(emb2[0].transpose(2,1)),2)\n",
    "            pooler_output2 = emb2[1]\n",
    "            pooler_output = self.dropout(torch.cat((pooler_output1, pooler_output2), 1))\n",
    "            pooler_output = nn.functional.relu(self.lin1(pooler_output))\n",
    "        else:\n",
    "            emb1 =self.emb1(data1)\n",
    "            #pooler_output1=torch.mean(emb1[0], 1)\n",
    "            #pooler_output1=torch.mean(self.pool(emb1[0].transpose(2,1)),2)\n",
    "            pooler_output1 = self.dropout(emb1[1])\n",
    "            pooler_output = nn.functional.relu(self.lin1(pooler_output1))\n",
    "\n",
    "        out = self.lin2(pooler_output)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_(model,loss,optimizer,dataloaders1,epoch,dataloaders2=None):\n",
    "    model.train()\n",
    "    allloss=[]\n",
    "    allbatch=[]\n",
    "    print('Train')\n",
    "    if args['science']==True:\n",
    "        for batch_idx, batch in tqdm(enumerate(zip(dataloaders1, dataloaders2))):\n",
    "            data1, mask1, target1 = batch[0]\n",
    "            data2, mask2, target2 = batch[1]\n",
    "            target1 = Variable(target1).cuda()\n",
    "            data1 = Variable(data1).cuda()\n",
    "            mask1=Variable(mask1).cuda()\n",
    "            target2 = Variable(target2).cuda()\n",
    "            data2 = Variable(data2).cuda()\n",
    "            mask2=Variable(mask2).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            out = model.forward(data1,mask1,data2,mask2)\n",
    "            lossall = loss(out,target1.float())            \n",
    "            lossall = torch.sum(lossall)\n",
    "            lossall.backward()\n",
    "            optimizer.step()\n",
    "            loss1=lossall.item()\n",
    "            allloss.append(loss1)\n",
    "            allbatch.append(batch_idx*epoch)\n",
    "           # pdb.set_trace()\n",
    "\n",
    "    else:\n",
    "        for batch_idx, batch in tqdm(enumerate(dataloaders1)):\n",
    "            data1, mask1, target1 = batch\n",
    "            target1 = Variable(target1).cuda()\n",
    "            data1 = Variable(data1).cuda()\n",
    "            mask1=Variable(mask1).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            out = model.forward(data1,mask1)\n",
    "            lossall = loss(out,target1.float())            \n",
    "            lossall = torch.sum(lossall)\n",
    "            lossall.backward()\n",
    "            optimizer.step()\n",
    "            allloss.append(lossall)\n",
    "            allbatch.append(batch_idx*epoch)\n",
    "    return allloss, allbatch\n",
    "def val_(model,dataloaders1,dataloaders2=None):\n",
    "    model.eval()\n",
    "    allout=[]\n",
    "    alltarget=[]\n",
    "    print('Validation')\n",
    "    if args['science']==True:\n",
    "        for batch_idx, batch in tqdm(enumerate(zip(dataloaders1, dataloaders2))):\n",
    "            data1, mask1, target1 = batch[0]\n",
    "            data2, mask2, target2 = batch[1]\n",
    "            target1 = Variable(target1).cuda()\n",
    "            data1 = Variable(data1).cuda()\n",
    "            mask1=Variable(mask1).cuda()\n",
    "            target2 = Variable(target2).cuda()\n",
    "            data2 = Variable(data2).cuda()\n",
    "            mask2=Variable(mask2).cuda()\n",
    "            with torch.no_grad(): \n",
    "                out = torch.nn.functional.sigmoid(model.forward(data1,mask1,data2,mask2))\n",
    "            out=out.cpu().detach().numpy()\n",
    "            target=target1.to('cpu').numpy().astype(int)\n",
    "            \n",
    "            allout.append(list(out))\n",
    "            alltarget.append(list(target))\n",
    "    else:\n",
    "        for batch_idx, batch in tqdm(enumerate(dataloaders1)):\n",
    "            data1, mask1, target1 = batch\n",
    "            target1 = Variable(target1).cuda()\n",
    "            data1 = Variable(data1).cuda()\n",
    "            mask1=Variable(mask1).cuda()\n",
    "            with torch.no_grad(): \n",
    "                out = torch.nn.functional.igmoid(model.forward(data1,mask1,target1))\n",
    "            out=out.cpu().detach().numpy()\n",
    "            target=target1.to('cpu').numpy().astype(int)\n",
    "            \n",
    "            allout.append(list(out))\n",
    "            alltarget.append(list(target))\n",
    "    return allout,alltarget\n",
    "def test_(model,dataloaders1,dataloaders2=None):\n",
    "    model.eval()\n",
    "    allout=[]\n",
    "    alltarget=[]\n",
    "    allattn=[]\n",
    "    print('test')\n",
    "    if args['science']==True:\n",
    "        for batch_idx, batch in tqdm(enumerate(zip(dataloaders1, dataloaders2))):\n",
    "            data1, mask1, target1 = batch[0]\n",
    "            data2, mask2, target2 = batch[1]\n",
    "            target1 = Variable(target1).cuda()\n",
    "            data1 = Variable(data1).cuda()\n",
    "            mask1=Variable(mask1).cuda()\n",
    "            target2 = Variable(target2).cuda()\n",
    "            data2 = Variable(data2).cuda()\n",
    "            mask2=Variable(mask2).cuda()\n",
    "            with torch.no_grad(): \n",
    "                out = torch.nn.functional.sigmoid(model.forward(data1,mask1,data2,mask2))\n",
    "            out=out.cpu().detach().numpy()\n",
    "            target=target1.to('cpu').numpy().astype(int)\n",
    "            \n",
    "            allout.append(out)\n",
    "            alltarget.append(target)\n",
    "            #allattn.append(attn)\n",
    "    else:\n",
    "        for batch_idx, batch in tqdm(enumerate(dataloaders1)):\n",
    "            data1, mask1, target1 = batch\n",
    "            target1 = Variable(target1).cuda()\n",
    "            data1 = Variable(data1).cuda()\n",
    "            mask1=Variable(mask1).cuda()\n",
    "            with torch.no_grad(): \n",
    "                out = torch.nn.functional.sigmoid(model.forward(data1,mask1))\n",
    "            out=out.cpu().detach().numpy()\n",
    "            target=target1.to('cpu').numpy().astype(int)\n",
    "            \n",
    "            allout.append(out)\n",
    "            alltarget.append(target)\n",
    "            #allattn.append(attn)\n",
    "    return allout,alltarget,\n",
    "def result(result):\n",
    "    alllist=[]\n",
    "    for i in result:\n",
    "        for j in i:\n",
    "            alllist.append(j)\n",
    "    return alllist\n",
    "\n",
    "\n",
    "torch.cuda.set_device(args[\"device\"])\n",
    "model= bert_rnn(args)\n",
    "model=model.to(args[\"device\"])\n",
    "params=model.parameters()\n",
    "optimizer = AdamW(params,lr = 2e-5, eps = 1e-8 )\n",
    "trainweight=torch.tensor(trainweight).float().cuda()\n",
    "loss = nn.BCEWithLogitsLoss(pos_weight=trainweight)\n",
    "alloss=[]\n",
    "allbatch=[]\n",
    "dev_lrl=1\n",
    "vallrl=[]\n",
    "testpred=[]\n",
    "testpid=[]\n",
    "current_early_stop_times=0\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    epochloss, epochbatch=train_(model,loss,optimizer,norloder[0],epoch,dataloaders2=sciloder[0])\n",
    "    alloss.append(epochloss)\n",
    "    allbatch.append(epochbatch)\n",
    "    #print('BCE training loss: ', np.mean(alloss))\n",
    "    allout,alltarget=val_(model,norloder[1],sciloder[1])\n",
    "    allout1=result(allout)\n",
    "    alltarget1=result(alltarget)\n",
    "    epochlrl=label_ranking_loss(alltarget1,allout1)\n",
    "    vallrl.append(epochlrl)\n",
    "    if epochlrl < dev_lrl:\n",
    "        print(\"- new best lrl{}\".format(epochlrl))\n",
    "        allout,alltarget=test_(model,norloder[2],sciloder[2])\n",
    "        allout=result(allout)\n",
    "        alltarget=result(alltarget)\n",
    "        #allattn=result(allattn)\n",
    "        allpd=pd.DataFrame(allout,columns=keylist)\n",
    "        allpd['newpid']=alltarget\n",
    "        os.makedirs(question, exist_ok=True)\n",
    "        allpd.to_csv(args['output_file_dir']+args['sentence_ranking_file'])\n",
    "        dev_lrl = epochlrl\n",
    "        current_early_stop_times = 0\n",
    "    else:\n",
    "        current_early_stop_times += 1\n",
    "        print(current_early_stop_times)\n",
    "    if current_early_stop_times >= args['early_stop_times'] :\n",
    "        break;\n",
    "print (\"- early stopping {} epochs without improvement\".format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
