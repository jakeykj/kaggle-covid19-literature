{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/dgunning/cord19.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part is using search_engine to find papers for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix meta_data_path_problem\n",
    "def pathfu (dataframe):\n",
    "    dataframe=dataframe.dropna()\n",
    "    seqs=dataframe.Path.values\n",
    "    titles=dataframe.title.values\n",
    "    allsq=[]\n",
    "    alltitle=[]\n",
    "    for seqnu in range(len(seqs)):\n",
    "        seq=str(seqs[seqnu])\n",
    "        s1q=seq.split(\"/\")\n",
    "        sq=[]\n",
    "        for i in range(7):\n",
    "            sq.append(s1q[i])\n",
    "        bases='/'\n",
    "        bases = bases.join(sq)\n",
    "        addsq=s1q[7].split(\";\")\n",
    "        for i in addsq:\n",
    "            i=i.replace(' ', '')\n",
    "            i=i.replace('.json', '')\n",
    "            base1=bases\n",
    "            allsq.append(base1+'/'+i+'.json')\n",
    "            alltitle.append(titles[seqnu])\n",
    "    returnframe=pd.DataFrame(allsq,columns=['path'])\n",
    "    returnframe['title']=alltitle\n",
    "    return returnframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from langdetect import detect\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up search engigne environment\n",
    "from cord import ResearchPapers, BIORXIV_MEDRXIV, CUSTOM_LICENSE\n",
    "from cord.jsonpaper import load_json_cache\n",
    "research_papers = ResearchPapers.load(data_dir='/home/COVID_NIH/data/',index='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create total docuemnt title, reading path, and abstract\n",
    "jasonpath=research_papers.get_json_paths()\n",
    "title=research_papers.metadata.title\n",
    "abstract=research_papers.metadata.abstract\n",
    "cord_uid=research_papers.metadata.cord_uid\n",
    "allinformation=pd.DataFrame(jasonpath,columns=['Path'])\n",
    "allinformation['abstract']=abstract\n",
    "allinformation['title']=title\n",
    "allinformation['pid']=cord_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchstr='inhibitor'\n",
    "fulltextfilename=\"inhibitor_NIH_full.csv\"\n",
    "fullabstractfilename=\"inhibitor_NIH_ab.csv\"\n",
    "fulljiffilename=\"inhibitor_NIH_jif.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get COVID title and abstract\n",
    "covinf=research_papers.covid_related().search(searchst,num_results=1000, covid_related=False,view='table').results[['cord_uid','title','abstract']]\n",
    "#get NON_COVID title and abstract\n",
    "notcovinf=research_papers.not_covid_related().search(searchst,num_results=10000, covid_related=False,view='table').results[['cord_uid','title','abstract']]\n",
    "\n",
    "print(len(covinf))\n",
    "print(len(notcovinf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get full abstract\n",
    "fullab=pd.concat([covinf,notcovinf])\n",
    "fullab=fullab.rename(columns={'cord_uid':'pid'})\n",
    "fullab=fullab[fullab.abstract!='']\n",
    "lan=[]\n",
    "# remove abstract not english\n",
    "for i in fullab.abstract:\n",
    "    lan1=detect(i)\n",
    "    lan.append(lan1)\n",
    "fullab['lan']=lan\n",
    "fullab=fullab[fullab.lan=='en']\n",
    "fullab=fullab[['pid','title','abstract']]\n",
    "#full_abstract\n",
    "fullab.to_csv(fullabstractfilename,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read impact factor file and get impact factor for each doc\n",
    "jif=pd.read_csv('/home/COVID_NIH/data/CORD-19-research-challenge/sjr.csv',sep=';')[['Title','Cites / Doc. (2years)']]\n",
    "jif=jif.rename(columns={'Title': 'journal', 'Cites / Doc. (2years)': 'IF'})\n",
    "jif['IF']=jif.IF.str.replace(\"\\\"\", '')\n",
    "jif['IF']=jif.IF.str.replace(\"\\,\" ,'.')\n",
    "jif['IF']=pd.to_numeric(jif['IF'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covid journal IF\n",
    "covidjif=research_papers.covid_related().search(searchst,num_results=1000, covid_related=False,view='table').results[['title','journal']]\n",
    "covidjif=covidjif.merge(jif,how='left')\n",
    "covidjif['newIF']=np.log(covidjif.IF)+1\n",
    "covidjif=covidjif[['title','IF']]\n",
    "# NON_covid journal IF\n",
    "noncovidjif=research_papers.not_covid_related().search(searchst,num_results=1000, covid_related=False,view='table').results[['title','journal']]\n",
    "noncovidjif=noncovidjif.merge(jif,how='left')\n",
    "noncovidjif['newIF']=np.log(noncovidjif.IF)+1\n",
    "noncovidjif=noncovidjif[['title','IF']]\n",
    "# full journal IF\n",
    "fulljif=pd.concat([covidjif,noncovidjif])\n",
    "fulljif.to_csv(fulljiffilename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get covid full_text_json_path\n",
    "covidallinf=allinformation[allinformation.title.isin(covinf.title)]\n",
    "#get NOn_covid full_text_json_path\n",
    "noncovidallinf=allinformation[allinformation.title.isin(notcovinf.title)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get covid full_text_json_path\n",
    "covidallinf=allinformation[allinformation.title.isin(covinf.title)]\n",
    "#get NOn_covid full_text_json_path\n",
    "noncovidallinf=allinformation[allinformation.title.isin(notcovinf.title)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_covid_full_text_frame\n",
    "returnframe=pathfu(covidallinf)\n",
    "covid_full_text=fulltextfu(returnframe,json)\n",
    "#get_NON_covid_full_text_frame\n",
    "returnframe=pathfu(noncovidallinf)\n",
    "non_covid_full_text=fulltextfu(returnframe,json)\n",
    "\n",
    "allframe=pd.concat([covid_full_text,non_covid_full_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allinformation1=allinformation[['title','pid']]\n",
    "# remove the meta date which title is blank\n",
    "allinformation1=allinformation1[allinformation1.title!='']\n",
    "allframe=allframe.merge(allinformation1[['title','pid']])\n",
    "allframe=allframe[allframe.text!='']\n",
    "lan=[]\n",
    "# remove text not english\n",
    "for i in allframe.text:\n",
    "    lan1=detect(i)\n",
    "    lan.append(lan1)\n",
    "allframe['lan']=lan\n",
    "allframe=allframe[allframe.lan=='en']\n",
    "allframe=allframe[['pid','title','text']]\n",
    "allframe.to_csv(fulltextfilename,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part is using spacy to find releated seed keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get each_sentence_entity from document, filter to UMLS, and count the total numbers of the entity in the document \n",
    "def entreportfu (dataframe,nlp):\n",
    "    alldata=pd.DataFrame()\n",
    "    allent=[]\n",
    "    for j in dataframe.abstract.values:\n",
    "        tempnlp=nlp(j)\n",
    "        tempentlist=list(tempnlp.ents)\n",
    "        for k in tempentlist:\n",
    "            if len(k._.umls_ents)>0:\n",
    "                allent.append(str(k))\n",
    "    uniqueent=list(set(allent))\n",
    "    allentnu=[]\n",
    "    entcategory=[]\n",
    "    for l in uniqueent:\n",
    "        entnu=allent.count(l)\n",
    "        allentnu.append(entnu)\n",
    "    temp=pd.DataFrame(uniqueent,columns=['entity'])\n",
    "    temp['entnucount']=allentnu\n",
    "    temp=temp.sort_values(by='entnucount',ascending=False)\n",
    "    alldata=pd.concat([alldata,temp])\n",
    "    return (alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import scispacy\n",
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "nlp = spacy.load('en_core_sci_lg')\n",
    "linker = UmlsEntityLinker(resolve_abbreviations=True)\n",
    "nlp.add_pipe(linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covidinfent=entreportfu(covinf,nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covidinfent.entity.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covidinfent[covidinfent.entity.str.contains('virus')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the previous section save to following three files \n",
    "#keylist.txt : the classname\n",
    "#valuelist.txt : the value of each classname\n",
    "#viruslist.txt the virus list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part snorkel_pseudo_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from snorkel.labeling import PandasLFApplier,LabelModel,LFAnalysis,LabelingFunction\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_raw_data (filepath):\n",
    "    allfile=pd.read_csv(filepath)\n",
    "    #get alldoucment\n",
    "    allfile['abstract']=allfile.abstract.astype(str)\n",
    "    #get allsentence\n",
    "    allsent=[]\n",
    "    allid=[]\n",
    "    for i in tqdm(range(len(allfile))):\n",
    "        temp=allfile.abstract.iloc[i]\n",
    "        temp=sent_tokenize(temp)\n",
    "        for j in range(len(temp)):\n",
    "            allsent.append(temp[j])\n",
    "            allid.append(allfile.pid.iloc[i])\n",
    "            \n",
    "    allsent=pd.DataFrame(allsent,columns=['sent'])\n",
    "    allsent['pid']=allid\n",
    "    \n",
    "    return allfile, allsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_labing(keylist,valuelist,virus):\n",
    "    def keyword_lookup(x, keywords, virus ,label):\n",
    "        if any(word in x.sent for word in keywords) and any(word in x.sent for word in virus) :\n",
    "            return label\n",
    "        return Norelevent\n",
    "\n",
    "    def make_keyword_lf(keywords, virus,name,label=None):\n",
    "        return LabelingFunction(\n",
    "            name=f\"keyword_{name}\",\n",
    "            f=keyword_lookup,\n",
    "            resources=dict(keywords=keywords,virus=virus ,label=label),\n",
    "        )\n",
    "    #This function has some drawback because I am writing the function to combine previous and latter sentence for a given sentence\n",
    "    def abstract_lookup(x, keywords,label):\n",
    "        if any(word in x.sent for word in keywords):\n",
    "            return label\n",
    "        return Norelevent\n",
    "\n",
    "    def make_abstract_lf(keywords,name,label=None):\n",
    "        return LabelingFunction(\n",
    "            name=f\"abstract_{name}\",\n",
    "            f=abstract_lookup,\n",
    "            resources=dict(keywords=keywords,label=label),\n",
    "        )\n",
    "    \n",
    "    Norelevent = -1\n",
    "    allweaklabf=[]\n",
    "    viruselist=virus\n",
    "    for i in range(len(keylist)):\n",
    "        vbname=keylist[i]\n",
    "        vbname1=keylist[i]+'ab'\n",
    "        vbnameab=vbname+'su'\n",
    "        vbnameab1=vbname+'su1'\n",
    "        labelvalue=i\n",
    "        vblabelname=vbname+'l'\n",
    "     \n",
    "        globals()[vbname] = make_keyword_lf(keywords=valuelist[i],virus=viruselist,name=vbnameab,label=labelvalue)\n",
    "        globals()[vbname1] = make_abstract_lf(keywords=valuelist[i],name=vbnameab1,label=labelvalue)\n",
    "        allweaklabf.append(globals()[vbname])\n",
    "        allweaklabf.append(globals()[vbname1])\n",
    "    \n",
    "    return allweaklabf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snorkel_process (keylist,dataframe,allweaklabf):\n",
    "    def func(x):\n",
    "        idx = (-x).argsort()[1:]\n",
    "        x[idx] = 0\n",
    "        return x\n",
    "    cardinalitynu=len(keylist)\n",
    "    applier = PandasLFApplier(lfs=allweaklabf)\n",
    "    all_train_l = applier.apply(df=dataframe)\n",
    "    report=LFAnalysis(L=all_train_l, lfs=allweaklabf).lf_summary()\n",
    "    print(report)\n",
    "    label_model = LabelModel(cardinality=cardinalitynu,verbose=False)\n",
    "    label_model.fit(all_train_l)\n",
    "    predt=label_model.predict(all_train_l)\n",
    "    predt1=label_model.predict_proba(all_train_l)\n",
    "    keylist1=keylist.copy()\n",
    "    predt2=pd.DataFrame(predt1,columns=keylist1 )\n",
    "    dataframe['L_label']=predt\n",
    "    dataframe1=dataframe.join(predt2, how='outer')\n",
    "    dataframe1=dataframe1[dataframe1.L_label>=0]\n",
    "  \n",
    "    train,test=train_test_split(dataframe1,test_size=0.2)\n",
    "    \n",
    "    trainsent=train.sent.values\n",
    "    \n",
    "    trainlabel=train[keylist].values\n",
    "    trainlabe2=trainlabel.copy()\n",
    "    np.apply_along_axis(func,  1, trainlabe2)\n",
    "    trainlabe2=np.where(trainlabe2 > 0, 1, 0)\n",
    "    testsent=test.sent.values\n",
    "    testlabel=test[keylist].values\n",
    "    testlabe2=testlabel.copy()\n",
    "    np.apply_along_axis(func,  1, testlabe2)\n",
    "    testlabe2=np.where(testlabe2 > 0, 1, 0)\n",
    "    return trainsent,trainlabel,valsent,vallabel,keylist,report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3269/3269 [00:02<00:00, 1393.50it/s]\n"
     ]
    }
   ],
   "source": [
    "allfile, allsent=build_raw_data('/home/text_download/inhibitor_NIH_ab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsent['newpid']=range(len(allsent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/home/text_download/keylist.txt', \"r\") as f:\n",
    "    alist =f.read().splitlines()\n",
    "    for line in alist:\n",
    "        keylist=line.split(',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuelist = []\n",
    "with open('/home/text_download/valuelist.txt', \"r\") as f:\n",
    "    alist =f.read().splitlines()\n",
    "    #alist =[x.lower() for x in alist]\n",
    "    for line in alist:\n",
    "        valuelist.append(line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/home/text_download/viruslist.txt', \"r\") as f:\n",
    "    alist =f.read().splitlines()\n",
    "    for line in alist:\n",
    "        viruslist=line.split(',')\n",
    "        #viruslist =[x.lower() for x in viruslist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "allweaklabf=loop_labing(keylist,valuelist,viruslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25739/25739 [00:42<00:00, 610.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               j Polarity  Coverage  Overlaps  Conflicts\n",
      "keyword_antihivsu              0      [0]  0.000272  0.000272   0.000000\n",
      "abstract_antihivsu1            1      [0]  0.001476  0.000505   0.000233\n",
      "keyword_antihcvsu              2      [1]  0.000117  0.000117   0.000000\n",
      "abstract_antihcvsu1            3      [1]  0.000311  0.000117   0.000000\n",
      "keyword_anti-parasitesu        4       []  0.000000  0.000000   0.000000\n",
      "abstract_anti-parasitesu1      5      [2]  0.000466  0.000039   0.000039\n",
      "keyword_immue_suppresivesu     6       []  0.000000  0.000000   0.000000\n",
      "abstract_immue_suppresivesu1   7      [3]  0.000427  0.000155   0.000155\n",
      "keyword_antivirussu            8      [4]  0.000350  0.000350   0.000000\n",
      "abstract_antivirussu1          9      [4]  0.003924  0.000544   0.000194\n",
      "keyword_interleukinsu         10      [5]  0.000155  0.000155   0.000000\n",
      "abstract_interleukinsu1       11      [5]  0.007693  0.000505   0.000350\n",
      "keyword_cancerdrugsu          12       []  0.000000  0.000000   0.000000\n",
      "abstract_cancerdrugsu1        13      [6]  0.000117  0.000078   0.000078\n",
      "keyword_receptorsu            14      [7]  0.001204  0.001204   0.000000\n",
      "abstract_receptorsu1          15      [7]  0.018144  0.001438   0.000233\n"
     ]
    }
   ],
   "source": [
    "trainsent,trainlabel,valsent,vallabel,keylist,report=snorkel_process (keylist,allsent,allweaklabf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(trainsent,columns=['sent']).to_csv(\"trainsent.csv\")\n",
    "pd.DataFrame(trainlabel,columns=keylist).to_csv(\"trainlabel.csv\" )\n",
    "pd.DataFrame(valsent,columns=['sent']).to_csv(\"valsent.csv\")\n",
    "pd.DataFrame(vallabel,columns=keylist).to_csv(\"vallabel.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
